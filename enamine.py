# -*- coding: utf-8 -*-
"""Enamine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13HaURRbPA7X4fMak9nYrRAX-U6riOurp
"""
import os
os.environ["OMP_NUM_THREADS"]="1"
os.environ["MKL_NUM_THREADS"]="1"
os.environ["OPENBLAS_NUM_THREADS"]="1"
os.environ["NUMEXPR_NUM_THREADS"]="1"

import sys
import argparse
import gc
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
import scipy
import itertools
from joblib import Parallel, delayed
from tqdm.auto import tqdm

import pyarrow as pa
from pyarrow.parquet import ParquetFile

import matplotlib.pyplot as plt
import seaborn as sns

from utils import props_and_fps_from_smiles

warnings.filterwarnings('ignore')

# RDKit imports
try:
    from rdkit import Chem
    from rdkit.Chem import (
        Descriptors, rdMolDescriptors, Lipinski, Crippen,
        rdFingerprintGenerator, DataStructs, Draw, rdDepictor, MACCSkeys
    )
    from rdkit.Chem.Draw import rdMolDraw2D
    from rdkit.Avalon import pyAvalonTools
    RDKIT_AVAILABLE = True
except ImportError:
    print("RDKit not available. Please install: pip install rdkit")
    RDKIT_AVAILABLE = False

## the smart files have umap fetaures and cluster assosiation
# smart_train_file = '/content/drive/MyDrive/drug_discovery_data/reversed_smart_filtered_100k_training.parquet'
# smart_test_file = '/content/drive/MyDrive/drug_discovery_data/reversed_test_like_validation.parquet'
train_file = './drug_discovery_data/crosstalk_train.parquet'
test_file = './drug_discovery_data/crosstalk_test.parquet'
test_file_new = './drug_discovery_data/crosstalk_test_inputs.parquet'


# using smart sampled data ID's
train_IDs = pd.read_csv('./drug_discovery_data/training_del_ids.csv')
val_IDs = pd.read_csv('./drug_discovery_data/validation_del_ids.csv')
train_sel = train_IDs['ID'].to_list()


def _process_batch_smiles(batch_smiles, start_idx):
    outs = Parallel(
        n_jobs=-1, backend="loky", prefer="processes",
        batch_size=256, pre_dispatch="3*n_jobs", max_nbytes=None
    )(delayed(props_and_fps_from_smiles)(smi, start_idx+i)
    for i, smi in enumerate(batch_smiles))
    return outs

class MemoryOptimizedSDFAnalyzer:
    """Memory-optimized SDF file analyzer that processes molecules in batches."""

    def __init__(self, sdf_file_path, output_dir="sdf_analysis_results"):
        self.sdf_file_path = Path(sdf_file_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # Initialize data storage (will be lists that we append to)
        self.properties = []
        self.fingerprints = {}
        self.errors = []

        # Fixed fingerprint generators with correct RDKit parameters
        self.fp_generators = {
            'ECFP4': rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048),
            # 'ECFP6': rdFingerprintGenerator.GetMorganGenerator(radius=3, fpSize=2048),
            # 'FCFP4': rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048, includeChirality=True),
            # 'FCFP6': rdFingerprintGenerator.GetMorganGenerator(radius=3, fpSize=2048, includeChirality=True),
            # 'MACCS': None,  # Special case - use MACCSkeys.GenMACCSKeys()
            # 'RDK': rdFingerprintGenerator.GetRDKitFPGenerator(),
            'AVALON': None  # Special case - use pyAvalonTools.GetAvalonFP()
            # 'ATOMPAIR': rdFingerprintGenerator.GetAtomPairGenerator(),
            # 'TOPTOR': rdFingerprintGenerator.GetTopologicalTorsionGenerator()
        }

        # Check disk space before starting
        self._check_disk_space()

    def _check_disk_space(self, required_gb=10):
        """Check if there's enough disk space."""
        try:
            import shutil
            free_space = shutil.disk_usage(self.output_dir).free
            free_gb = free_space / (1024**3)

            if free_gb < required_gb:
                print(f"WARNING: Low disk space. Need {required_gb}GB, have {free_gb:.1f}GB")
                print("Consider reducing max_molecules or using a different output directory")
            else:
                print(f"Disk space check passed: {free_gb:.1f}GB available")
        except Exception as e:
            print(f"Could not check disk space: {e}")

    def count_molecules(self):
        """Count the number of molecules in the SDF file."""
        print("Counting molecules in SDF file...")
        count = 0
        with open(self.sdf_file_path, 'r') as f:
            for line in f:
                if line.strip() == '$$$$':
                    count += 1
        print(f"Total molecules in SDF file: {count:,}")
        return count

    # def process_molecules_in_batches(self, max_molecules=None, batch_size=1000):
    #     """
    #     Process molecules in batches to avoid memory issues.

    #     Args:
    #         max_molecules (int): Maximum number of molecules to process
    #         batch_size (int): Number of molecules to process in each batch
    #     """
    #     print(f"Processing molecules in batches of {batch_size}")

    #     supplier = Chem.SDMolSupplier(str(self.sdf_file_path))
    #     total_molecules = self.count_molecules()

    #     if max_molecules:
    #         total_molecules = min(total_molecules, max_molecules)

    #     processed = 0
    #     batch_count = 0

    #     # Initialize fingerprint storage
    #     for fp_name in self.fp_generators.keys():
    #         self.fingerprints[fp_name] = []

    #     while processed < total_molecules:
    #         batch_count += 1
    #         print(f"\nProcessing batch {batch_count}...")

    #         # Process one batch
    #         batch_properties, batch_fingerprints = self._process_batch(
    #             supplier, batch_size, processed, total_molecules
    #         )

    #         # Append to main storage
    #         self.properties.extend(batch_properties)
    #         for fp_name, fps in batch_fingerprints.items():
    #             self.fingerprints[fp_name].extend(fps)

    #         processed += len(batch_properties)

    #         # Save intermediate results every 10 batches
    #         if batch_count % 10 == 0:
    #             self._save_intermediate_results(batch_count)
    #             gc.collect()  # Force garbage collection

    #         print(f"Processed {processed:,}/{total_molecules:,} molecules")

    #     # Convert fingerprint lists to numpy arrays
    #     for fp_name in self.fp_generators.keys():
    #         if self.fingerprints[fp_name]:
    #             self.fingerprints[fp_name] = np.array(self.fingerprints[fp_name])

    #     print(f"Successfully processed {len(self.properties):,} molecules")
    #     print(f"Failed to process {len(self.errors):,} molecules")


    def _mol_props_and_fps(self, mol, idx):
        props = self._get_molecular_properties(mol, idx)
        fps_dict = {}
        for fp_name, generator in self.fp_generators.items():
            try:
                if fp_name == 'AVALON':
                    bv = pyAvalonTools.GetAvalonFP(mol, nBits=2048)
                else:
                    bv = generator.GetFingerprint(mol)  # ExplicitBitVect
                arr = np.zeros((2048,), dtype=np.int8)
                DataStructs.ConvertToNumpyArray(bv, arr)
                fps_dict[fp_name] = arr
            except Exception as e:
                self.errors.append(f"{fp_name} fail at {idx}: {e}")
                fps_dict[fp_name] = np.zeros(2048, dtype=np.int8)
        return props, fps_dict


    def process_molecules_in_batches(self, max_molecules=None, batch_size=5000):
        supplier = Chem.SDMolSupplier(str(self.sdf_file_path))
        processed = 0
        self.fingerprints = {"ECFP4": [], "AVALON": []}
        self.properties = []

        while True:
            batch = [m for _, m in zip(range(batch_size), supplier) if m is not None]
            if not batch: break
            if max_molecules and processed >= max_molecules: break
            if max_molecules:
                over = processed + len(batch) - max_molecules
                if over > 0: batch = batch[:-over]
            batch_smiles = [Chem.MolToSmiles(m) for m in batch]

            for props, fps in _process_batch_smiles(batch_smiles, processed):
                self.properties.append(props)
                for k, v in fps.items():
                    if v is not None:
                        self.fingerprints[k].append(v)
            processed += len(batch)

    def _process_batch(self, supplier, batch_size, start_idx, max_molecules):
        """Process a single batch of molecules."""
        batch_properties = []
        batch_fingerprints = {fp_name: [] for fp_name in self.fp_generators.keys()}

        molecules_processed = 0
        current_idx = start_idx

        while molecules_processed < batch_size and current_idx < max_molecules:
            try:
                mol = supplier[current_idx]
                current_idx += 1

                if mol is not None:
                    # Calculate properties
                    props = self._get_molecular_properties(mol, current_idx - 1)
                    batch_properties.append(props)

                    # Generate fingerprints
                    for fp_name, generator in self.fp_generators.items():
                        try:
                            if fp_name == 'MACCS':
                                # Special handling for MACCS keys
                                fp = MACCSkeys.GenMACCSKeys(mol)
                                fp_array = np.array(fp)
                            elif fp_name == 'AVALON':
                                # Special handling for AVALON fingerprints
                                fp = pyAvalonTools.GetAvalonFP(mol, nBits=2048)
                                fp_array = np.array(fp)
                            else:
                                # Standard fingerprint generation
                                fp = generator.GetFingerprint(mol)
                                fp_array = np.array(fp)
                            batch_fingerprints[fp_name].append(fp_array)
                        except Exception as e:
                            self.errors.append(f"Error generating {fp_name} for molecule {current_idx - 1}: {str(e)}")
                            batch_fingerprints[fp_name].append(np.zeros(2048))

                    molecules_processed += 1
                else:
                    self.errors.append(f"Failed to load molecule at index {current_idx - 1}")

            except Exception as e:
                self.errors.append(f"Error processing molecule {current_idx - 1}: {str(e)}")
                current_idx += 1

        return batch_properties, batch_fingerprints

    def _get_molecular_properties(self, mol, mol_id):
        """Calculate molecular properties for a single molecule."""
        try:
            # Basic properties
            smiles = Chem.MolToSmiles(mol)
            mw = Descriptors.MolWt(mol)
            logp = Descriptors.MolLogP(mol)
            hbd = Descriptors.NumHDonors(mol)
            hba = Descriptors.NumHAcceptors(mol)
            psa = Descriptors.TPSA(mol)
            rotbonds = Descriptors.NumRotatableBonds(mol)
            aromatic_rings = Descriptors.RingCount(mol)
            heavy_atoms = Descriptors.HeavyAtomCount(mol)
            formal_charge = Chem.rdmolops.GetFormalCharge(mol)

            # Lipinski's Rule of Five violations
            lipinski_violations = 0
            if mw > 500:
                lipinski_violations += 1
            if logp > 5:
                lipinski_violations += 1
            if hbd > 5:
                lipinski_violations += 1
            if hba > 10:
                lipinski_violations += 1

            return {
                'molecule_id': mol_id,
                'smiles': smiles,
                'mw': mw,
                'logp': logp,
                'hbd': hbd,
                'hba': hba,
                'psa': psa,
                'rotbonds': rotbonds,
                'aromatic_rings': aromatic_rings,
                'heavy_atoms': heavy_atoms,
                'formal_charge': formal_charge,
                'lipinski_violations': lipinski_violations,
                'valid': True
            }
        except Exception as e:
            return {
                'molecule_id': mol_id,
                'smiles': '',
                'mw': np.nan,
                'logp': np.nan,
                'hbd': np.nan,
                'hba': np.nan,
                'psa': np.nan,
                'rotbonds': np.nan,
                'aromatic_rings': np.nan,
                'heavy_atoms': np.nan,
                'formal_charge': np.nan,
                'lipinski_violations': np.nan,
                'valid': False
            }

    def _save_intermediate_results(self, batch_count):
        """Save intermediate results to avoid data loss using sparse storage."""
        print(f"Saving intermediate results after batch {batch_count}...")

        # Save properties
        if self.properties:
            props_df = pd.DataFrame(self.properties)
            props_file = self.output_dir / f"molecular_properties_batch_{batch_count}.csv"
            props_df.to_csv(props_file, index=False)

        # Save fingerprints as sparse matrices to save disk space
        for fp_name, fps in self.fingerprints.items():
            if fps:
                try:
                    from scipy.sparse import csr_matrix, save_npz
                    sparse_fps = csr_matrix(fps)
                    fp_file = self.output_dir / f"{fp_name}_sparse_batch_{batch_count}.npz"
                    save_npz(fp_file, sparse_fps)
                    print(f"Saved {fp_name} as sparse matrix (saves ~80% disk space)")
                except ImportError:
                    # Fallback to regular numpy save if scipy not available
                    fp_file = self.output_dir / f"{fp_name}_fingerprints_batch_{batch_count}.npy"
                    np.save(fp_file, np.array(fps))
                    print(f"Saved {fp_name} as regular numpy array")

    def finalize_results(self):
        """Finalize and save all results."""
        print("Finalizing results...")

        # Convert properties to DataFrame
        self.properties = pd.DataFrame(self.properties)

        # Save final properties
        properties_file = self.output_dir / "molecular_properties_final.csv"
        self.properties.to_csv(properties_file, index=False)
        print(f"Final properties saved to {properties_file}")

        # Save final fingerprints as sparse matrices to save disk space
        for fp_name, fps in self.fingerprints.items():
            if len(fps) > 0:
                try:
                    from scipy.sparse import csr_matrix, save_npz
                    sparse_fps = csr_matrix(fps)
                    fp_file = self.output_dir / f"{fp_name}_sparse_final.npz"
                    save_npz(fp_file, sparse_fps)
                    # sparse_fps.dump(fp_file)
                    print(f"Final {fp_name} fingerprints saved as sparse matrix to {fp_file}")
                except ImportError:
                    # Fallback to regular numpy save if scipy not available
                    fp_file = self.output_dir / f"{fp_name}_fingerprints_final.npy"
                    np.save(fp_file, fps)
                    print(f"Final {fp_name} fingerprints saved to {fp_file}")

    def create_summary_statistics(self):
        """Create summary statistics and visualizations."""
        print("Creating summary statistics...")

        if len(self.properties) == 0:
            print("No properties available for analysis")
            return

        # Basic statistics
        stats = self.properties.describe()
        print("\n=== MOLECULAR PROPERTIES SUMMARY ===")
        print(stats)

        # Save statistics
        stats_file = self.output_dir / "summary_statistics.csv"
        stats.to_csv(stats_file)

        # Create visualizations
        self._create_visualizations()

    def _create_visualizations(self):
        """Create molecular property visualizations."""
        print("Creating visualizations...")

        # Set up the plotting style
        plt.style.use('default')
        sns.set_palette("husl")

        # Create figure with subplots
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Molecular Properties Distribution', fontsize=16)

        # Plot distributions
        properties_to_plot = ['mw', 'logp', 'hbd', 'hba', 'psa', 'rotbonds']

        for i, prop in enumerate(properties_to_plot):
            row = i // 3
            col = i % 3
            ax = axes[row, col]

            if prop in self.properties.columns:
                data = self.properties[prop].dropna()
                if len(data) > 0:
                    ax.hist(data, bins=50, alpha=0.7, edgecolor='black')
                    ax.set_xlabel(prop.upper())
                    ax.set_ylabel('Frequency')
                    ax.set_title(f'Distribution of {prop.upper()}')

                    # Add statistics
                    mean_val = data.mean()
                    median_val = data.median()
                    ax.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')
                    ax.axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.2f}')
                    ax.legend()

        plt.tight_layout()

        # Save the plot
        plot_file = self.output_dir / "molecular_properties_distribution.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        print(f"Visualization saved to {plot_file}")
        plt.show()

    def export_for_ml_workflow(self):
        """Export data in format compatible with existing ML workflow."""
        print("Exporting data for ML workflow integration...")

        if len(self.properties) == 0:
            print("No properties available for export")
            return

        # Create a combined dataset similar to the parquet format
        ml_data = self.properties.copy()

        # Add fingerprint data as comma-separated strings (matching parquet format)
        for fp_name, fp_data in self.fingerprints.items():
            if len(fp_data) > 0:
                # Convert fingerprints to comma-separated strings
                fp_strings = [','.join(fp.astype(str)) for fp in fp_data]
                ml_data[f'{fp_name}'] = fp_strings

        # Add some additional columns for compatibility
        ml_data['ID'] = range(len(ml_data))
        ml_data['DEL_ID'] = [f"ENAMINE_{i:08d}" for i in range(len(ml_data))]
        ml_data['DELLabel'] = 0  # Default label
        ml_data['RawCount'] = 1  # Default count
        ml_data['Target'] = 'Unknown'  # Default target

        # Rename columns to match expected format
        column_mapping = {
            'mw': 'MW',
            'logp': 'ALOGP',
            'hbd': 'hb_donors',
            'hba': 'hb_acceptors',
            'psa': 'PSA',
            'rotbonds': 'rotating_bonds',
            'aromatic_rings': 'aromatic_rings',
            'heavy_atoms': 'heavy_atoms',
            'formal_charge': 'formal_charge',
            'lipinski_violations': 'lipinski_violations'
        }

        ml_data = ml_data.rename(columns=column_mapping)

        # Save as parquet for compatibility
        parquet_file = self.output_dir / "sdf_analysis_results.parquet"
        ml_data.to_parquet(parquet_file, index=False)
        print(f"ML-compatible data saved to {parquet_file}")

        return ml_data

    def run_analysis(self, max_molecules=None, batch_size=1000):
        """Run the complete analysis pipeline."""
        print("Starting memory-optimized SDF analysis...")

        # Process molecules in batches
        self.process_molecules_in_batches(max_molecules=max_molecules, batch_size=batch_size)

        if len(self.properties) == 0:
            print("No molecules processed. Exiting.")
            return

        # Finalize results
        self.finalize_results()

        # Create visualizations
        self.create_summary_statistics()

        # Export for ML workflow
        ml_data = self.export_for_ml_workflow()

        print(f"\n=== ANALYSIS COMPLETE ===")
        print(f"Results saved to: {self.output_dir}")
        print(f"Total molecules processed: {len(self.properties)}")
        print(f"Valid molecules: {len(self.properties[self.properties['valid']])}")
        print(f"Errors encountered: {len(self.errors)}")

        return ml_data

"""Main function to run the memory-optimized SDF analyzer."""

sdf_file_path = './drug_discovery_data/Enamine_screening_collection_202508.sdf'
input = Path(sdf_file_path)
output_file_path = './drug_discovery_data/sdf_analysis_results'
output = Path(output_file_path)
output.mkdir(exist_ok=True)
max_molecules = None
batch_size = 100000

if not os.path.exists(input):
    print(f"Error: SDF file {input} not found")
    sys.exit(1)

# Create analyzer
analyzer = MemoryOptimizedSDFAnalyzer(input, output)

# Run analysis
try:
    ml_data = analyzer.run_analysis(
        max_molecules=max_molecules,
        batch_size=batch_size
    )
    print("Analysis completed successfully!")

except Exception as e:
    print(f"Error during analysis: {str(e)}")
    sys.exit(1)

"""# Basic Dataloader"""

## changed the basic dataloader to read the ID's from the csv file to create train and validation set - and also be able to load any column like AVALON etc
def basic_dataloader(
    filepath, x_col, y_col='DELLabel', ids = None, max_to_load=1000, chunk_size=5000
):
    """
    Loads data from a Parquet file into memory, optionally as a sparse matrix.

    Args:
        filepath (str): Path to the Parquet file. This is the location of your data file on disk.
        x_col (str): Name of the feature column. This column should contain your input features as comma-separated strings.
        y_col (str, optional): Name of the label column. If None, only features are loaded. Defaults to 'DELLabel'. This column contains the target values (labels) for supervised learning.
        max_to_load (int, optional): Number of rows to load. If None, loads all rows. Defaults to 1000. Use this to work with a smaller sample of your data.
        chunk_size (int, optional): Number of rows to read at a time from disk. Defaults to 1000. This controls memory usage when loading large files.
        sparse (bool, optional): If True, returns a scipy sparse matrix for X. Defaults to False.

    Returns:
        X (np.ndarray or scipy.sparse.csr_matrix): Feature matrix.
        y (np.ndarray or None): Label array if y_col is provided, else None.
    """

    pf = ParquetFile(filepath)
    if ids is not None:
        columns = ['ID'] + [x_col] + ([y_col] if y_col is not None else [])
    else:
        columns = [x_col] + ([y_col] if y_col is not None else [])

    if max_to_load is None:
        max_to_load = pf.metadata.num_rows

    print("Filtering data by target IDs...")
    filtered_data = []
    y_list = []
    loaded = 0

    for batch in tqdm(pf.iter_batches(columns=columns, batch_size=chunk_size),
                      desc='Loading and filtering chunks'):
        batch_df = pa.Table.from_batches([batch]).to_pandas()
        remaining = max_to_load - loaded
        if len(batch_df) > remaining:
          batch_df = batch_df.iloc[:remaining]

        # Filter by target IDs
        # batch_df['ID'] = batch_df['ID'].astype(str)
        if ids is not None:
            batch_df = batch_df[batch_df['ID'].isin(ids)]
        if len(batch_df) > 0:
            # Convert feature column to matrix
            # exploded = batch_df[x_col].str.split(',', expand=True).astype(float, copy=False)
            # filtered_data.append(scipy.sparse.csr_matrix(exploded))
            vals = batch_df[x_col].values
            mat = scipy.sparse.csr_matrix(
                np.fromstring(",".join(vals), sep=",", dtype=float).reshape(len(vals), -1)
            )
            filtered_data.append(mat)

            if y_col is not None:
                y_list.append(batch_df[y_col].values)
            loaded += len(batch_df)
        del batch_df, exploded

    # Combine filtered data
    if filtered_data:
        X = scipy.sparse.vstack(filtered_data)
        if y_col is not None and y_list:
            y = np.concatenate(y_list)
            print(f"Loaded {X.shape[0]} samples after filtering")
            return X, y
        else:
            print(f"Loaded {X.shape[0]} samples after filtering")
            return X
    else:
        print("No matching IDs found in the data")
        return None, None

"""# Training data using ID's and different features"""

# Estimate xgboost and catboost Model Performance Confidence Intervals a baggig model

from sklearn.model_selection import KFold, cross_val_score, train_test_split
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import cross_val_predict
import numpy as np
from eval import BinaryEvaluator

def train_ensemble_model(X_tr,Y_tr, model_type):
  # Create an ensemble of XGBoost models

  if model_type == "xgboost":
    ensemble_model = BaggingClassifier(
        XGBClassifier(random_state=42),
        n_estimators=10,
        random_state=42,
    )
  elif model_type == "catboost":
    params = {
                'random_strength': 2, # only non-default hyperparam, default is 1
                'random_seed': 1234,
                'verbose': 0,
                'loss_function': 'Logloss',
                'task_type': 'GPU',
                'devices': '0'
            }
    ensemble_model = BaggingClassifier(
        CatBoostClassifier(**params),
        n_estimators=10,
        random_state=42,
        n_jobs=-1
    )

  ensemble_model.fit(X_tr, Y_tr)

  return ensemble_model

def _pred_one(est, X):
    return est.predict(X), est.predict_proba(X)[:,1]

def get_predictions(ensemble_model, X):
    outs = Parallel(n_jobs=-1, prefer="threads")(delayed(_pred_one)(est, X)
                                                 for est in ensemble_model.estimators_)
    preds = np.stack([o[0] for o in outs], axis=0)
    probs = np.stack([o[1] for o in outs], axis=0)

    mean_prediction = preds.mean(axis=0)
    ci_low  = np.percentile(preds, 2.5, axis=0)
    ci_high = np.percentile(preds,97.5, axis=0)

    # Majority vote for Y_pred; mean for Y_prob
    Y_pred = (mean_prediction>=0.5).astype(int)
    Y_prob = np.column_stack([1-probs.mean(axis=0), probs.mean(axis=0)])
    return Y_pred, Y_prob, mean_prediction, ci_low, ci_high

def evaluate(X, Y, Y_prob):
  eval = BinaryEvaluator(X.toarray(), Y)
  metric_dict = eval.compute_metrics(yt=Y, yp=Y_prob[:,1]) # or validation

  for metric_name, metric_val in metric_dict.items():
      print(f'{metric_name:20s}: {metric_val:.2f}')

  return metric_dict

def train_ensemble_model_for_feature(feature, model):

    X, Y = basic_dataloader(train_file, x_col= feature, y_col = "DELLabel", ids = train_sel, max_to_load = None, chunk_size=10000)
    X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=0.10, random_state=42)

    X_te_val, Y_te_val = basic_dataloader(train_file, x_col=feature, y_col = "DELLabel", ids = val_sel, max_to_load = None, chunk_size=10000)
    X_te = basic_dataloader(test_file_new, x_col=feature, y_col = None, ids = None, max_to_load = None, chunk_size=10000)

    ensemble_model = train_ensemble_model(X_tr,Y_tr,model)

    for name,X,Y in zip (['Train like val', 'Test like val', 'Test'],[X_val, X_te_val, X_te],[Y_val, Y_te_val, None]):

      Y_pred, Y_prob, mean_prediction, ci_low, ci_high = get_predictions(ensemble_model, X)

      print(f'Performance on {name}')
      if Y is not None:
        evaluate(X, Y, Y_prob)
      else:
        None
        return Y_pred, Y_prob, mean_prediction, ci_low, ci_high

## using smart sampled data ID's that based on ECFP4 features
train_IDs = pd.read_csv('./drug_discovery_data/training_del_ids.csv')
val_IDs = pd.read_csv('./drug_discovery_data/validation_del_ids.csv')
train_sel = train_IDs['ID'].to_list()
val_sel = val_IDs['ID'].to_list()

model = 'catboost'; feature = 'AVALON'
# for feature in ['AVALON', 'ATOMPAIR', 'ECFP4',	'ECFP6',	'FCFP4',	'FCFP6',	'MACCS',	'RDK', 'TOPTOR', 'ATOMPAIR']:

X, Y = basic_dataloader(train_file, x_col= feature, y_col = "DELLabel", ids = train_sel, max_to_load = None, chunk_size=10000)
X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=0.10, random_state=42)

X_te_val, Y_te_val = basic_dataloader(train_file, x_col=feature, y_col = "DELLabel", ids = val_sel, max_to_load = None, chunk_size=10000)
X_te = basic_dataloader(test_file_new, x_col=feature, y_col = None, ids = None, max_to_load = None, chunk_size=10000)

ensemble_model = train_ensemble_model(X_tr,Y_tr,model)

for name,X,Y in zip (['Train like val', 'Test like val'],[X_val, X_te_val],[Y_val, Y_te_val]):

  Y_pred, Y_prob, mean_prediction, ci_low, ci_high = get_predictions(ensemble_model, X)
  print(f'Performance on {name}')
  evaluate(X, Y, Y_prob)
  print('\n')

Y_pred, Y_prob, mean_prediction, ci_low, ci_high = get_predictions(ensemble_model, X_te)

pf = ParquetFile(test_file_new)
preds = pf.read(columns = ['RandomID']).to_pandas()
preds['DELLabel'] = Y_prob[:,1]

team_name = 'dataloaders'
preds.to_csv(f'{team_name}_{feature}_{model}.csv', index=False)